import logging
import sys
import os
import base64
import io
import time
import asyncio

from PyPDF2 import PdfReader
from ping3 import ping

from mcp.client.session import ClientSession
from mcp.client.sse import  sse_client

from openai import OpenAI


from agents import Agent, Runner, gen_trace_id, trace
from agents.mcp import MCPServer, MCPServerSse
from agents.model_settings import ModelSettings

from fastapi import FastAPI
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    final_output: str

app = FastAPI()
openai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
app.mount("/static", StaticFiles(directory="frontend"), name="static")
logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(levelname)s:     [applogs] %(message)s')
activities_assistant_id = ""
activities_thread_id = ""
chat_history


@app.get("/", response_class=HTMLResponse)
async def serve_index():
    global activities_assistant_id 
    global chat_history

    # RESOURCES
    file_contents = await get_resources()

    # TOOLS
    tools = await get_tools()

    chat_history = []



    #vector_store_id = upload_file_to_vector("rulebook vector store", pdf_file)
    #logging.info(f"vector_store_id: {vector_store_id}")

    activities_assistant_id = create_assistant(tools=tools, file_contents=file_contents)

    with open("frontend/index.html", "r") as f:
        return HTMLResponse(content=f.read())
    
    

@app.post("/chat")
async def chat_endpoint(chat: ChatRequest):
    global activities_assistant_id 
    try:
        
        result = await openai_agent_family_activities(chat.message)

        return {"response": result}

    except Exception as e:
        logging.info(f"Exception: {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/prompts")
async def get_prompt_suggestions():
    return {
        "prompts": await get_prompts()
    }

async def openai_agent_family_activities(message: str) -> str:
    
    activities_run_id = run_threads(message=message)

    await wait_for_completion(activities_run_id)

    # Now get the messages from the thread
    message_content = get_assistant_reply(run_id=activities_run_id)
    
    return message_content.value

# UTILITIES METHODS
        
def upload_file_to_vector(vector_store_name: str, file_obj) -> str:
    # Create a vector store caled "Financial Statements"
    vector_store = openai.vector_stores.create(name=vector_store_name)

    # Use the upload and poll SDK helper to upload the files, add them to the vector store,
    # and poll the status of the file batch for completion.
    file_batch = openai.vector_stores.file_batches.upload_and_poll(
        vector_store_id=vector_store.id, 
        files=[file_obj]
    )
    return vector_store.id

def create_assistant(tools: any, file_contents: str) -> str:
    assistant = openai.beta.assistants.create(
        name="Family Activities Assistant",
        instructions=f"""
You are a Family Activity Planner, you use tools and GPT-4o architecture.  
Your have a CSV file that contains all activities you want to present.

When you see a message that contains a question about activities you are to 
generate responses based on the CSV list and use tools based on the question.

Follow these guidance:

1. If no location is given then assume the location is Fort Worth, TX
2. if you don't know how many adults and kids and the kids ages then assume that this is for 1 adult and 0 kids.
3. In your response repeat the location if providing distance information
4. In your response repeat kids and adult count if being asked about total cost

Your personality is to be funny and charming. make sure your personality follows these guidelines:
- Be informative and comprehensive.
- Do not take more than 30 seconds to process
- if you can't come up with an answer ask questions or state why you couldn't answer the question.

Here is the CSV contents: 

{file_contents}
        """,
        tools=tools,
        model="gpt-4o",  # or gpt-4-turbo
    )
    logging.info(f"assistant created: {assistant}")
    return assistant.id

def run_threads(message: str) -> str:
    global activities_thread_id

    if activities_thread_id == "":
        threads = openai.beta.threads.create()
        activities_thread_id = threads.id

    openai.beta.threads.messages.create(
        thread_id=activities_thread_id,
        role="user",
        content=message
    )
    logging.info(f"thread created: {activities_thread_id}")
    
    run = openai.beta.threads.runs.create(
        thread_id=activities_thread_id,
        assistant_id=activities_assistant_id
    )
    logging.info(f"run created: {run}")
    
    return run.id

async def wait_for_completion(run_id: str):
    global activities_thread_id
    start_time = time.time()
    timeout = 120  # seconds

    while True:
        run_status = openai.beta.threads.runs.retrieve(
            thread_id=activities_thread_id,
            run_id=run_id
        )
        if run_status.status in ["completed", "failed", "cancelled"]:
            break

        if time.time() - start_time > timeout:
            logging.warning("Timeout reached. Cancelling the run...")
            openai.beta.threads.runs.cancel(
                thread_id=activities_thread_id,
                run_id=run_id
            )
            raise TimeoutError("The run was cancelled after exceeding the timeout.")

        await asyncio.sleep(1)

    logging.info(f"Run done: {run_status.status}")


def get_assistant_reply(run_id: str) -> any:
    global activities_thread_id
    messages = list(openai.beta.threads.messages.list(thread_id=activities_thread_id, run_id=run_id))
    logging.info(f"run done: {messages}")


    # The assistant's reply is usually the last message
    message_content = messages[0].content[0].text
    return message_content

def mcp_tool_to_openai(tool: dict) -> dict:
    return {
        "type": "function",
        "function": {
            "name": tool.name,
            "description": tool.description,
            "parameters": tool.inputSchema,
        }
    }

async def get_tools() -> list[any]:
    async with MCPServerSse(
        name="Activities Server",
        params={
            "url": "http://mcp-server:8001/sse",
        },
    ) as mcp_server:
        trace_id = gen_trace_id()
        with trace(workflow_name="Activities Server", trace_id=trace_id):
            print(f"View trace: https://platform.openai.com/traces/trace?trace_id={trace_id}\n")
            tools = await mcp_server.list_tools()
            print(tools)
            openai_tools = [mcp_tool_to_openai(tool) for tool in tools]
            print(openai_tools)
            return openai_tools


async def get_resources() -> str:
    async with sse_client( url="http://mcp-server:8001/sse" ) as (read, write):
        logging.info("Connected.")
        async with ClientSession(read, write) as session:
            await session.initialize()
            resources_result = await session.list_resources()
            logging.info(f"what are the resources {resources_result}")
            first_uri = resources_result.resources[0].uri
            read_result = await session.read_resource(uri=first_uri)
            return read_result.contents[0].text 
        


async def get_prompts() -> str:
    async with sse_client( url="http://mcp-server:8001/sse" ) as (read, write):
        logging.info("Connected.")
        async with ClientSession(read, write) as session:
            await session.initialize()
            prompt_results = await session.list_prompts()
            logging.info(f"What are the prompts: {prompt_results}")

            prompt_texts = []
            for prompt in prompt_results.prompts:
                prompt_text = await session.get_prompt(prompt.name)
                logging.info(f"prompt_text: {prompt_text}")
                prompt_texts.append(prompt_text.messages[0].content.text)

            return prompt_texts
        